\section{Basic Probability Theory}

\subsection{Sample Space and Events}

\vspace{\parskip}

\begin{definition}[Probability Space and Sample Space] \index{probability space} \index{sample space} \index{outcome} \index{event}
    A \textit{\textbf{probability space}} $(\Omega, \Prob)$ consists of a finite or countable set $\Omega$ called the sample space, and the probability function $\Prob:\; \Omega \to \R$ such that for all $\omega \in \Omega$, $\Prob(\omega) \geq 0$ and $\sum_{\omega \in \Omega} \Prob(\omega) = 1$. We cann an element $\omega \in \Omega$ a sample point, or \textit{\textbf{outcome}}, or \textit{\textbf{simple event}}.
\end{definition}

A sample space models some random experiment, where $\Omega$ contains all possible outcomes, and $\Prob(\omega)$ is the probability of the outcome $\omega$. We always talk about probabilities in relation to a sample space.

\begin{definition}[Event]
    An event $A$ is a set of outcomes, $A \subseteq \Omega$. We define the probability of an event $A$ to be the sum of the probabilities of its elements
    $$
    \Prob(A) = \sum_{\omega \in A} \Prob(\omega)
    $$ 
\end{definition}

If we have $\Prob(\omega) = \Prob(\omega')$ for all distinct $\omega, \omega' \in \Omega$, we say that the probability is uniform over $\Omega$.

\subsection{Properties of Probability Functions}

\vspace{\parskip}

\begin{definition}[Complement]
    The complement of an event $A$ is
    $$
    \overline{A} = \Omega \setminus A
    $$
    The complement of $A$ is also denoted by $\text{not }A$. 
\end{definition}

\begin{theorem}
    $$
    \begin{aligned}
        \Prob(\overline{A}) &= 1 - \Prob(A) \\
        \Prob(A \cup B) &= \Prob(A) + \Prob(B) - \Prob(A \cap B) \\
        \Prob(A \cup B) &\leq \Prob(A) + \Prob(B)
    \end{aligned}
    $$
    It can then be proved by induction that
    $$
    \Prob(A_1 \cup A_2 \cup \cdots \cup A_k) \leq \Prob(A_1) + \Prob(A_2) + \cdots + \Prob(A_k)
    $$
    for any events $A_1, \cdots A_k$.
\end{theorem}

We say that two events $A$ and $B$ are disjoint or mutually exclusive if $A \cap B = \emptyset$. If $A$ and $B$ are disjoint, $\Prob(A \cup B) = \Prob(A) + \Prob(B)$.

\section{Conditional Probability and Independence}

Conditional probability allows us to compute the probability of some event given that we already know that some other events have occurred.

\begin{definition}[Conditional Probability] \index{conditional probability}
    The probability of an event $A$ conditional on an event $B$ is
    $$
    \Prob(A \mid B) = \frac{\Prob(A \cap B)}{\Prob(B)}
    $$
    given that $\Prob(B) > 0$. 
\end{definition}

\section{Average Case Analysis}

Let $A$ be an algorithm, and let
\[
t(x) = \text{number of steps taken by $A$ on input $x$}
.\] 

We know that the worst case time complexity of $A$ is
$$
T(n) = \max \{ t(x) \mid \text{$x$ has size $n$ } \}
$$

We can define the average case time complexity of $A$ as

\begin{definition}
    The average case time complexity of $A$ is
    $$
    T'(n) = \Expected [ t(x) \mid \text{$x$ has size $n$} ]
    $$
    If all inputs of size $n$ are equally likely, then
    $$
    T'(n) = \frac{\sum \{ t(x) \mid size(x) = n \}}{| x \mid size(x) = n |}
    $$
\end{definition}

In general, the average case time complexity is less than or equal to the worst case time complexity, that is $T'(n) \leq T(n)$ for all $n$.

The average case time complexity is dependent on the probability distribution of the inputs, which in turn can depends on the application where the algorithm is to be used. However, this is usually unknown, in which case, uniform distribution usually makes the analysis easier. For some applications, an algorithm with good average case behavior but bad worst case behavior is sufficient.

We follow the following steps before we can analyze the average case time complexity of an algorithm:

\begin{itemize}
    \item define the sample space of the inputs
    \item define the probability distribution function
    \item define any necessary random variables
\end{itemize}

\section{Average Case Analysis of Linear Search}

Consider the following algorithm for linear search in an unsorted array.

\begin{codebox}
    \Procname{$\proc{Linear-Search}(L,k)$}
    \li $j = 1$
    \li \While $j \leq n$ \Do
    \li \If $L[k] == k$
    \li \Then \Return $j$ \End
    \li $j = j + 1$ \End
    \li \Return 0 
\end{codebox}

It is obvious that the worst case complexity is $O(n)$ because there will be at most $n$ comparisons with the searching key $k$. Following the procedure introduced above, we can perform an average case complexity analysis.

\begin{enumerate}
    \item \textbf{Define sample space}: This will be the set of possible inputs that we are considering in our analysis. We define our sample space to be all pairs $(L,k)$ where $L$ is an input of size $n$ and $k$ is a key. Some observations on our choice of sample space:
    \begin{itemize}
        \item If we fix $L = [l_1, \cdots, l_n]$, then the sample space for $k$ is $[l_1,\cdots,l_n, \textsc{nil}]$ where $\textsc{nil}$ is used to indicate a value that is not in $L$. Hence, for a $L$ of size $n$, the sample space for $k$ contains $n+1$ sample points.
        \item The number of comparisons performed by $\proc{Linear-Search}(L,k)$ is the same for $(L,k)$ and $(L',k')$ if $k$ occurs in $L$ at the same position as $k'$ in $L'$.
        \item If $k$ and $k'$ are not in $L$, then $\proc{Linear-Search}(L,k)$ will also perform the same number of comparisons.
        \item The relative order between elements of $L$ does not matter since $\proc{Linear-Search}$ only performs equality tests, so choosing $L=[1,\cdots,n]$ is just as good as $L=[n,\cdots,1]$.
    \end{itemize}
    Hence, we can formally define our sample space as
    $$
    S_n = \{ (L,i) \mid i \in \{ \textsc{nil}, 1, \cdots, n \} \}
    $$
    where $L = [1,\cdots,n]$.
    \item \textbf{Probability distribution}: We choose uniform distribution, which means that every point in the sample space has the same probability $\displaystyle \frac{1}{n+1}$.
    \item \textbf{Define random variable}: Let $t_n:\; S_n \to \N$ be such that $t_n(i)$ is the number of comparisons performed when the input is $(L,i)$ where
    $$
    t_n(i) = \begin{cases}
        i & \text{for $i=1,\cdots,n$ } \\
        n & \text{for $i=0$ }
    \end{cases}
    $$
    \item \textbf{Analysis}:

    For the uniform distribution,
    $$
    \begin{aligned}
        T'(n) &= \Expected[t_n] = \frac{\sum \{ t_n(L,i) \mid i = \textsc{nil},\cdots,n \}}{n+1} \\
        &= \frac{n + \sum_{i=1}^n i}{n+1} \\
        &= \frac{n + \frac{n(n+1)}{2}}{n+1} \\
        &= \frac{n}{2} + \frac{n}{n+1} \\
        &< \frac{n}{2} + 1 
    \end{aligned}
    $$
    Hence, $\displaystyle T'(n) \in O(\frac{n}{2})$.
\end{enumerate}

\section{Average Case Analysis of Quick Sort}

Quicksort is ued to sort a multi-set of elements $S$ from a totally ordered domain. The pseudocode for quicksort is shown below.

\begin{codebox}
    \Procname{$\proc{Quicksort}(S)$}
    \li \If $|S| \leq 1$
    \li \Then \Return $S$ \End
    \li $pivot = \text{select a pivot}$
    \zi \Comment partition $S$ into $L$, $E$, and $G$ such that $L$ contains all elements less than $pivot$
    \zi \Comment $E$ contains all elements equal to $pivot$, and $G$ contains all elements greater than $pivot$
    \li $L,E,G = \proc{Partition}(S,pivot)$
    \li \Return $\proc{Quicksort}(L) + E + \proc{Quicksort}(G)$
\end{codebox}

In practice, we often choose the first element of $S$ as the pivot. The worst case time complexity of quicksort is $O(n^2)$ because it has to partition the input $S$ into three parts, $L$, $E$, and $G$, which gives us this recurrence.
$$
T(n) = T(n-1) + T(0) + \Omega(n) \in O(n^2)
$$
Following the procedure introduced above, we can perform an average case complexity analysis. In \proc{Quicksort}, only the relative order of the elements matter, not their actual values.

\begin{enumerate}
    \item Define sample space: $S_n = \text{all permutations of $\{1,\cdots,n\}$}$
    \item Probability distribution: We know that $\left| S_n \right| = n!$. Assuming uniform distribution, we have
    $$
    \Prob[\pi] = \frac{1}{n!} \qquad \text{for all $\pi \in S_n$}
    $$
    \item Random variables: Let $t_n: \; S_n \to \N$ be the random variable such that
    $$
    t_n(i) = \text{number of element comparison performed by $\proc{Quicksort}(S)$}
    $$
    \item Analysis: Let $T'(n) = \Expected[t_n]$. Let $X_{ij} \; S_n \to \{0,1\}$ be an indicator random variable such that $X_{ij}(\pi) = 1$ if and only if elements $i$ and $j$ are compared during $\proc{Quicksort}(\pi)$. Since no pair of elements is compared more than once,
    $$
    t_n(\pi) = \sum_{1 \leq i < j \leq n} X_{ij}(\pi),
    $$
    and
    $$
    \begin{aligned}
        T'(n) = \Expected[t_n] &= \sum_{1 \leq i < j \leq n} \Expected[X_{ij}] & {\text{by linearity of expectation }} \\
        &= \sum_{1 \leq i < j \leq n} \Prob[X_{ij} = 1] & {\text{since $X_{ij}$ is an indicator variable }}
    \end{aligned}
    $$
    In $\proc{Quicksort}(\pi)$, as long as no element in $\{i, \cdots, j\}$ is chosen as a pivot, these elements will stay together, either all going to $L$, or all going to $G$ in recursive calls. 

    Eventually, one of the elements in $\{i, \cdots, j\}$ is chosen as pivot. Suppose that $p$ is the first of the elements in $\{ i, \cdots, j \}$ that is chosen as pivot. If $i < p < j$, then $i$ and $j$ are not compared during $\proc{Quicksort}$. If $p=i$ or $j=j$, then $i$ and $j$ are compared.
    
    There are $j-i+1$ possibilities for $p$ to be chosen as pivot. Among those possible choices, $X_{ij} = 1$ for two possibilities, and $X_{ij} = 0$ for the rest. Since all permutations of the inputs are equally likely, each of these possibilities for $p$ is equally like and has probability of $\displaystyle \frac{1}{j-i+1}$.

    Hence,
    $$
    \Prob[X_{ij} = 1] = \frac{2}{j-i+1}
    $$

    Using this, we can rewrite $T'(n)$ as with $k = j-i+1$ 
    $$
    \begin{aligned}
        T'(n) &= \Expected[t_n] \\
        &= \sum_{1 \leq i < j \leq n} \Prob[X_{ij} = 1] \\
        &= \sum_{i=1}^n \sum_{j=i+1}^{n-1} \frac{2}{j-i+1} \\
        &= \sum_{i=1}^n \sum_{k=1}^{n-1} \frac{2}{k+1} & \text{substituting $k$} \\
        &< \sum_{i=1}^n \sum_{k=1}^n \frac{2}{k} \\
        &= \sum_{i=1}^n O(n\, \lg n) & \text{harmonic series} \\
        &= O(\lg n)
    \end{aligned}
    $$

    Therefore, the average case time complexity of quicksort is $O(n \, \lg n)$.
\end{enumerate}

From this analysis, we notice that the average case time complexity for quicksort is much smaller than the worst case time complexity. If the actual distribution of inputs is close to uniform distribution, then the average case time complexity will serve as a more realistic estimate of running time. But problem arises when the actual distribution is not uniform. To solve this issue, we can modify the quicksort algorithm to use a randomized pivot selection.

\section{Randomized Quicksort}

In randomized quicksort, we randomly pick the pivot instead of using the first element of $S$. Pick each element of $S$ to be the pivot with probability of $\displaystyle \frac{1}{|S|}$ using a random number generator. 

\begin{codebox}
    \Procname{$\proc{Randomized-Quicksort}(S)$}
    \li \If $|S| \leq 1$
    \li \Then \Return $S$ \End
    \li $r = \proc{Random}(1,|S|)$
    \li $pivot = S[r]$ 
    \zi \Comment partition $S$ into $L$, $E$, and $G$ such that $L$ contains all elements less than $pivot$
    \zi \Comment $E$ contains all elements equal to $pivot$, and $G$ contains all elements greater than $pivot$
    \li $L,E,G = \proc{Partition}(S,pivot)$
    \li \Return $\proc{Randomized-Quicksort}(L) + E + \proc{Randomized-Quicksort}(G)$
\end{codebox}

$\proc{Random}(i,j)$ will return a number in $\{i, i+1, \cdots, j-1, j\}$ each equally likely, assuming $i \leq j$.

Let $t(I,p)$ denote the running time of algorithm $A$ on input $I$ with the sequence of random choices $p$. Then, the expected running time of algorithm $A$ on input $I$ is
$$
\Expected_p \left[ t(T,p) \right] = \sum_e \Prob [p] t(I,p)
$$

The worst case expected time complexity of algorithm $A$ is
$$
T(n) = \max_{I \in S_n} \Expected_p \left[ t(I,p) \right]
$$
where $S_n$ is the set of all inputs of size $n$.

Note that the worst case complexity does not depend on any assumption about the input distribution.