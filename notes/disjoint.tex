\section{Disjoint Set ADT}

A disjoint set is a collection of nonempty, pairwise disjoint sets of objects. Each set contains 1 special element called its representative. Sometimes, it may also be referred to as Union-Find.

A disjoint set ADT should support the following operations:

\begin{itemize}
    \item $\proc{Make-Set}(x)$: Takes an object $x$ that is not in any of the current sets and adds $\{ x \}$ into the collection. The representative of the new set is $x$.
    \item $\proc{Find-Set}(x)$: Given an element $x$ (the pointer to $x$), return the representative of the set that contains $x$.
    \item $\proc{Union}(x,y)$: Given (the pointers to) two elements $s \in S_x$ and $y \in S_y$, adds the set $S_x \cup S_y$ to the collection and removes $S_x$ and $S_y$ from the collection. Return the representative of the new set, which can be any element of the set. If $x$ and $y$ belong to the same set, $\proc{Union}(x,y)$ has no effect.
    \item $\proc{Link}(x,y)$: Same as $\proc{Union}$, except that $x$ and $y$ are the representatives of their sets.
\end{itemize}

Disjoint sets can be useful for the following applications:

\begin{itemize}
    \item Classifying webpages and finding sets of duplicate pages;
    \item Reconstructing DNA sequences from small fragments (genome assembly);
    \item Graph algorithms such as finding connected components in a graph;
    \item Determining if two finite automata accepts the same language;
    \item Model checking for program verification.
\end{itemize}

\section{Data Structures for Disjoint Sets}

Consider a sequence of $m$ $\proc{Make-Set}$, $\proc{Link}$, and $\proc{Find-Set}$ operations applied to an initially empty colleciton of sets. Let $n \leq m$ be the number of $\proc{Make-Set}$ operations in this sequence. Each $\proc{Union}(x,y)$ operation can be replaced by

\begin{codebox}
    \li $x' = \proc{Find-Set}(x)$
    \li $y' = \proc{Find-Set}(y)$
    \li $\proc{Link}(x',y')$
\end{codebox}

In this section, we will take a look at a few data structures that implement the disjoint set ADT. For all the data structures described in this section, we would also need an auxillary data structure to keep track of the set/element pointers.

\subsection{Circular Linked List}

We can implement a disjiont set using a circular linked list. Each element of the disjoint set is a linked list node which contains a \id{next} pointer and a bit indicating whether this element is the representative of the set it belongs to. We can then represent each set in the collection by a circular singly linked list of its element nodes. In each linked list, exactly one node stores a 1 bit, indicating that node is the representative.

\begin{codebox}
    \Procname{$\proc{Make-Set}(x)$ }
    \zi Create a linked list where $x$ is the only node
    \zi Set this node to 1 so that $x$ is the representative
\end{codebox}

Clearly, $\proc{Make-Set}$ runs in $O(1)$ time.

\begin{codebox}
    \Procname{$\proc{Find-Set}(x)$ }
    \zi Follow pointers starting from $x$ until an element with bit 1 is found. 
    \zi Return the element with bit 1
\end{codebox}

The complexity of $\proc{Find-Set}$ is $O(n)$ assuming all $n$ elements are in the same set, or more generally, $O(\text{size of the set containing $x$})$.

\begin{codebox}
    \Procname{$\proc{Link}(x,y)$ }
    \zi \If $x \neq y$ \Then
    \zi Change the first element of each list to point to the second element 
    \zi of the other list \End
    \zi The representative of the new set is $x$ 
\end{codebox}

$\proc{Link}(x,y)$ would also run in $O(1)$ time.

The worst-case sequence complexity of a sequence of $m$ $\proc{Make-Set}$, $\proc{Link}$, and $\proc{Find-Set}$ operations is $\Theta(mn)$. The upper bound follows from the fact that if each set has $O(n)$ elements, each operation takes $O(n)$ time and there are $m$ operations. To obtain the matching lower bound, we perform $n$ $\proc{Make-Set}$ operations, create one set by performing $n-1$ \proc{Link} operations, then finally perform $m-2n+1$ \proc{Find-Set} operations on the element that follows the representative in the list. Each of these operation takes $\Omega(2n-1)+\Omega(n(2m-2n+1)) = \Omega(mn)$ time.

\subsection{Linked List With Back Pointers}

Represent each set by a singly linked list of its elements. The first element in the list is the set representative. Each node contains a \id{next} pointer of a \id{representative} pointer.

For $\proc{Make-Set}(x)$, simply create a node with both the \id{next} and \id{representative} pointers pointing to itself. This takes $O(1)$ time.

For $\proc{Find-Set}(x)$, follow the representative pointer in $x$, and this also takes $O(1)$ time.

\begin{codebox}
    \Procname{$\proc{Link}(x,y)$}
    \zi Insert the list with representative $y$ immediately after $x$
    \zi Update the representative pointers of each element in list containing $y$ to point to $x$
\end{codebox}

This will take $O(\text{length of list containing $y$})$.

Now, let us consider the worst-case sequence complexity of the sequence of operations described at the beginning of the section (i.e. a sequence of $m$ \proc{Make-Set}, \proc{Link}, and \proc{Find-Set} operations and $n$ is the number of \proc{Make-Set} operations in the sequence). The worst-case sequence complexity is $\Theta(m+n^2)$. For the upper bound, there can be at most $n-1$ non-trivial \proc{Link} operations that takes $O(n)$ steps. All other operations takes $O(1)$ steps. For the lower bound, perform $n$ \proc{Make-Set} operations, followed by $n-1$ \proc{Link} operations. The \proc{Link} operations take in total $\sum_{i=2}^{n} (i-1) \in \Omega(n^2)$. In addition, there are $m$ operations, each of which takes $\Omega(1)$ time. Hence, the lower bound of the worst-case sequence complexity is $\Omega(n^2+m)$. 

The only expensive operation is \proc{Link}. If we call \proc{Link} on lists $y$ of length $\Theta(n)$, then the time would be $O(mn)$. This is an overestimate since $\proc{Link}(x,y)$ with $x \neq y$ can occur at most $n-1$ times.


\subsection{Linked List With Back Pointers Using Union By Weight}

In addition to the \id{back} and \id{representative} pointer, we store the size of each set at the head of the list.

The time to perform \proc{Link} is $O(\min \{ |S_x|,\, |S_y|\})$. During \proc{Link}, the representative of the bigger set becomes the representative of the new set.

We claim that with union by weight, the worst-case sequence complexity is $\Omega(m+n \log n)$. For the lower bound, we can first perform $n=2^k$ \proc{Make-Set} operations, and then repeatedly \proc{Link} sets of equal sizes. There are $n/2 = 2^{k-1}$ pair of lists each of length $2^0$, and $n/4 = 2^{k-2}$ pairs of lists each of length $2^1$, etc. The total cost is $\sum_{i=1}^k \frac{n}{2^i} \cdot 2^{i-1} = k\cdot n/2 \in \Omega(n \log n)$. 

For the upper bound, consider the number of times the \id{representative} pointer changes for each element $x$. Whenever $x$'s representative pointer is changed, $|S_x|$ is at least doubled. With $|S_x| = 1$ initially, the maximum size of $S_x$ is $n$. Hence, $x$'s representative pointer changes at most by $\log_2 n$ times.

\subsection{Trees}

We can represent each set by a tree, where the root is the representative and each node has a \id{parent} pointer points to its parent. The root points to itself. There are no child pointers.

\begin{codebox}
    \Procname{$\proc{Find-Set}(x)$}
    \zi Follow the parent pointers from $x$ until reaching the root
\end{codebox}

This clearly takes $\Theta(\attrib{T}{height})$ time.

\begin{codebox}
    \Procname{$\proc{Link}(x,y)$}
    \zi Change the parent pointer to point to $x$
    \zi Return $x$
\end{codebox}

This takes $O(1)$ time.

The worst-case sequence complexity is $\Omega(mn)$. To obtain a lower bound, we can make $n$ sets by calling \proc{Make-Set} $n$ times. Then, link these sets by calling \proc{Link} $n-1$ time. Because of the implementation of \proc{Link}, this will essentially just create a linked list of length $n-1$. Finally, call \proc{Find-Set} $m-(2n-1)$ times, which in total gives us a $\Omega(mn)$ lower bound.

For the upper bound, each operation is $O(h) \leq O(n)$. With $m$ operations, it takes $O(mn)$ in total. This is the same as a circular linked list, so this does not improve the performance. In fact, it might be better if we use a linked list with back pointers using union by height.

\subsection{Trees With Union By Height}

Store the height of each tree at the root. Representative of the taller tree becomes the new representative. If the heights are different, then the new height remains the maximum of the height of the two. If the two trees have the same height, the height of the new tree increases by 1.

\begin{lemma}
    Any tree of height $h$ has size of at least $2^h$.
\end{lemma}

\begin{proof}
    By induction on $h$.
\end{proof}

\begin{corollary}
    Any tree created by a sequence of $n$ \proc{Make-Set} and at most $n-1$ \proc{Link} operations has $O(\log n)$ height.
\end{corollary}

The upper bound on the worst-case sequence complexity is $O(m \log n)$ since the worst-case for \proc{Find-Set} is $O(\log n)$ and there are $m$ operations in total. The lower bound can be achieved by constructing a binomial tree and then perform \proc{Find-Set} on a deepest node. So the worst-case sequence complexity is $\Theta(m \log n)$.

\subsection{Trees With Path Compression}

During $\proc{Find-Set}(x)$, each node visited on the path from $x$ to the root is made a child of the root. This at most doubles the running time of \proc{Find-Set}.

\textit{Claim}. The worst-case sequence complexity is 
$$
\Theta \left( n + f \cdot \left( \frac{1+ \log n}{\log (2 + \frac{f}{n})} \right) \right)
$$
where $f$ is the number of \proc{Find-Set} operations.

\subsection{Additional Implementations}

Trees with union by weight and path compression. 

Trees with union by height and path compression. The problem with this is that we cannot trivially maintain height information in the data structure when path compression is performed because path compression can change the height.

\subsection{Trees With Union By Rank and Path Compression}

In this implementation, we store the rank of each tree instead of its height. The rank can be thought as a ``pseudoheight'' that denotes the height of the tree if there is no path compression.

\begin{codebox}
    \Procname{$\proc{Link}(x,y)$}
    \li \If $\proc{Rank}(x) \isequal \proc{Rank}(y)$ \Then
        \li $\attrib{y}{parent} = x$
        \li $\proc{Rank}(x) = \proc{Rank}(x) + 1$
    \li \ElseIf  $\proc{Rank}(x) > \proc{Rank}(y)$ \Then
        \li $\attrib{y}{parent} = x$
    \li \Else
        \li $\attrib{x}{parent} = y$
\end{codebox}

The rank of any tree created by a sequence of $\proc{Make-Set}$ and $\proc{Link}$ operations is equal to its height and hence is in $O(\log n)$.

\proc{Find-Set} operations with path compression reduce the height of the tree but do not change the ranks. Maintain the rank of a tree takes fewer bits than maintaining the weight of a tree.

\begin{codebox}
    \Procname{$\proc{Find-Set}(x)$}
    \li \If $x \neq \attrib{x}{p}$ \Then
        \li $\attrib{x}{p} = \proc{Find-Set}(\attrib{x}{p})$
        \End
    \li \Return $\attrib{x}{p}$ 
\end{codebox}

Finally, let us examine the sequence complexity of a sequence of operations performed on a disjoint set implemented using union-by-rank with path compression.

\begin{definition}[Iterated Logarithm]
    $$
    \log^*(n) = \min \{ i \geq 0 \mid \log^{(i)} (n) \leq 1 \}
    $$
    where $\log^{(0)}(n) = n$, $\log^{(1)}(n) = \log n$, and $\log^{(i+1)}(n) = \log(\log^{(i)})n$. Intuitively, $\log^*$ is the number of times we need to apply $\log$ to $n$ until we get a number of at most 1. For most practical purposes, $\log^*$ with base 2 has at most 5 (e.g. $\log^* 2^{65536} = 5$).
\end{definition}

\begin{theorem}
    The worst-case sequence complexity of a sequence of $m$ \proc{Make-Set} operations and \proc{Find-Set} operations with $n$ \proc{Make-Set} operations performed on a disjoint set implemented using union-by-rank with path compression is $O(m \log^* n)$. 
\end{theorem}

\begin{theorem}[A Tight Bound on Union-by-Rank With Path Compression]
    The worst-case sequence complexity of a disjoint set implemented using union-by-rank with path compression is $\Theta(m\alpha(n,m))$ where $\alpha$ is the inverse Ackermann function.
\end{theorem}

\section{Amortized Analysis of Disjoint Set}

The following proof was adopted from \textit{Introduction to Algorithms (1st Edition)} by Cormen, Leiserson, and Rivest. The proof was first presented by Hopcroft and Ullman in the paper \textit{Set Merging Algorithms} in SIAM J. Compt. in 1973.

\begin{lemma} \label{lem:logstar1}
    Any tree of rank $r$ has size of at least $2^r$.
\end{lemma}

\begin{proof}
    By induction on the number of \proc{Link} operations.
\end{proof}

\begin{lemma} \label{lem:logstar2}
    If $p$ is the parent of $x$, then $\id{rank}(p) > \id{rank}(x)$. 
\end{lemma}

\begin{proof}
    By construction and the fact that rank is non-decreasing. The non-decreasing property of rank can be shown using induction on the number of operations.
\end{proof}

\begin{lemma} \label{lem:logstar3}
    For any integer $r \geq 0$, there are at most $n/2^r$ nodes of rank $r$.
\end{lemma}

\begin{proof}
    Fix $r \geq 0$. The trees rooted at any two nodes with rank $r$ are disjoint. By Lemma \ref{lem:logstar1}, each of these trees contains at least $2^r$ nodes. Since there are at most $n$ nodes in total, there are at most $n/2^r$ nodes of rank $r$.
\end{proof}

\begin{corollary} \label{cor:logstar4}
    Every node has rank at most $\lfloor \lg n \rfloor$ 
\end{corollary}

\begin{proof}
    Let $r > \lg n$, then there are at most $n/2^r < 1$ nodes of rank $r$.
\end{proof}

Now we proceed to prove Theorem.

\begin{proof}
    We partition the ranks into $\log^* n$ blocks where rank $r$ goes into block $\log^* r$.

    For example,
    \begin{itemize}
        \item Block 0: rank 0 and 1
        \item Block 1: rank 2
        \item Block 2: rank $3=2^1 + 1$  to $4 = 2^2$ 
        \item Block 3: rank $5=2^2 + 1$  to $16=2^{2^2}$ 
        \item Block $j$: rank $T(j-1)+1$ to $T(j)$ where $T(j)$ denotes a height $j$ tower of 2. 
    \end{itemize}

    Note that the last block is Block $\log^*(\log n) = \log^*n - 1$.
    
    The analysis uses the aggregate method. Consider the charges corresponding to the actual cost of each operation. \proc{Make-Set} and \proc{Link} are each associated with one charge since both are $O(1)$ operation.

    For \proc{Find-Set}, we associate it with two types of charges: block charge and path charge. Suppose that the \proc{Find-Set} operation starts at a node $x_0$ and the path it takes to the root $x_l$ consists of nodes $x_0,x_1,\ldots,x_l$, where each of $x_i$ is the parent of $x_{i-1}$. By Lemma \ref{lem:logstar2}, we know that $\id{rank}(x_0) < \id{rank}(x_1) < \cdots < \id{rank}(x_l)$.
    
    There is one block charge for the last node in the sequence $x_0 \ldots x_l$ whose rank is in Block $j$ for $j\geq 0$. Additionally, give a block charge to the child of the root $r_{l-1}$, it it does not already have a block charge. This will cover the cost of calling \proc{Find-Set} on the child of root.

    All other nodes have a path charge. Note that by definition, $x_l$ has a block charge. For example, if we have the find path $x_0,x_1,x_2,x_3,x_4,x_5$ with ranks $2,3,4,6,8,9$, respectively, the block charges and path charges are assigned as follows ($\Box$ represents a block charge, $\rightsquigarrow$ represents a path charge).

    \begin{tabular}{c||cc|c|cc|ccccccc}
        &&& $\color{red}\Box$ & $\color{red}\rightsquigarrow$ & $\color{red}\Box$ & & $\color{red}\rightsquigarrow$ && $\color{red}\Box$ & $\color{red}\Box$ \\
        Node & & & $x_0$ & $x_1$ & $x_2$ & & $x_3$ & & $x_4$ & $x_5$ \\
        \hline
        Rank & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & $\cdots$ & 16 \\
        Block & \multicolumn{2}{c|}{Block 0} & Block 1 & \multicolumn{2}{c|}{Block 2} & \multicolumn{7}{c}{Block 3}
    \end{tabular}

    Node that the total number of charges is still equal to the total number of nodes visited in all \proc{Find-Set} operations in the sequence of $m$ operations.

    In each \proc{Find-Set} operation, there is at most 1 block charge per block, plus one for the child of root. There are $\log^* n$ blocks, so there are at most $m[\log^*(n) + 1] \in O(m \log^* n)$ block charges. Whenever we call \proc{Find-Set} on a child of the root, the cost is paid for by the block charges.

    If $x$ is not a root or a child of the root, then $\id{rank}(x)$ does not change during path compression. But it gains a parent of a larger rank. This implies the observation that if $x$ is not root or child of the root and $x$ gets a block charge in some \proc{Find-Set} operation, then it gets a block charge in all subsequent \proc{Find-Set} operations involving $x$. Once a node gets a block charge, it will no longer receive path charges.

    Suppose that $\id{rank}(x)$ is in Block $j$, we want to know how many times $x$ can get a path charge. Each time \proc{Find-Set} is called on $x$, $x$ gets a parent with larger rank. Eventually, $x$ will get a parent with rank in the next block and hence gets a block charge ($x$ now sits on the boundary between two blocks). Recall that ranks contained in each block range from $T(j-1)+1$ to $T(j)$. It follows that when $j=0,2$, $x$ get at most 1 path charge; when $j=1$, $x$ gets no block charge; and for all $j > 2$, at most $T(j)-T(j-1)-1$ path charges. Once $x$ gets the final path charge, it will gets a block charge and no more path charge in subsequent calls. 

    Next, we bound the number of nodes that have ranks in Block $j$ in order to derive a bound on the number of path charges. Let $N(j)$ be the number of nodes whose ranks are in Block $j$. By Lemma \ref{lem:logstar3}
    $$
    N(j) \leq \sum_{r=T(j-1)+1}^{T(j)} \frac{n}{2^r}
    $$
    For $j \geq 1$,
    $$
    \begin{aligned}
        N(j) &\leq \frac{n}{2^{T(j-1)+1}} \sum_{r=0}^{T(j)-T(j-1)-1} \frac{1}{2^r} \\
        &< \frac{n}{2^{T(j-1)+1}} \sum_{r=0}^\infty \frac{1}{2^r} \\
        &= \frac{n}{2^{T(j-1)}} \\
        &= \frac{n}{T(j)}
    \end{aligned}
    $$
    Finally, we can conclude that the total number path charges by summing over all blocks the product of the maximum number of nodes with ranks in that block and the maximum number of path charges per node in that block.
    $$
    \begin{aligned}
        \sum_{j=0}^{\log^* n - 1} N(j) \cdot (T(j)-T(j-1)-1) &\leq \sum_{j=0}^{\log^* n - 1} N(j) \cdot T(j) \\
        &\leq \sum_{j=0}^{\log^* n - 1} \frac{n}{T(j)} \cdot T(j) \\
        &= n \log^* n \in O(m \log^* n)
    \end{aligned}
    $$
\end{proof}

A proof of the tight bound is presented in CLRS Chapter 21.4 using the potential method.

\subsection{An Alternative Proof}

I personally found this proof using block charges and path charges rather counterintuitive. Here, I present a more intuitive and straightforward proof, first discovered by Seidel and Sharir in their paper \textit{Top-down Analysis of Path Compression}, published in SIAM J. Compt. in 2005.